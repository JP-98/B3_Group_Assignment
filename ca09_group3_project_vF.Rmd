---
title: "Group B3: AirBnB Analytics in Vienna"
date: "18 Oct 2021"
author: "Study group 3 // Ismail Rihai, John Purcell, Parthivi Bansal, Ivo Margetich, Xinyi Liu, Xinyue Zhang, Jacopo Lorusso Caputi"
output:
  html_document:
    highlight: zenburn
    theme: flatly
    toc: yes
    toc_float: yes
    number_sections: yes
    code_folding: show
---


```{r setup, include=FALSE}
# leave this chunk alone
options(knitr.table.format = "html") 
knitr::opts_chunk$set(warning = FALSE, message = FALSE, 
  comment = NA, dpi = 300)
options(warn=-1)
knitr::opts_chunk$set(message = FALSE)
Sys.setlocale("LC_MESSAGES", 'en_GB.UTF-8')
Sys.setenv(LANG = "en_US.UTF-8")
```


```{r load-libraries, echo=FALSE, warning=FALSE,message=FALSE}
library(tidyverse) # the usual stuff: dplyr, readr, and other goodies
library(lubridate) # to handle dates
library(GGally) # for correlation-scatter plot matrix
library(ggfortify) # to produce residual diagnostic plots
library(rsample) # to split dataframe in training- & testing sets
library(janitor) # clean_names()
library(broom) # use broom:augment() to get tidy table with regression output, residuals, etc
library(huxtable) # to get summary table of all models produced
library(kableExtra) # for formatting tables
library(moderndive) # for getting regression tables
library(skimr) # for skim
library(mosaic)
library(leaflet) # for interactive HTML maps
library(tidytext)
library(viridis)
library(vroom)
library(geosphere)
library(ggrepel)
library(rsample)
library(lemon)
library(ggthemes)
knit_print.data.frame <- lemon_print
```
Executive Summary
===================================================================================================================================

## Introduction

This coursework focuses on applying a multivariate linear regression model to predict the total cost for two travelers staying for four nights in an Airbnb in Vienna, Austria. The data set is quite large and raw (non-numerical values, variables with several NA values etc.), so this is as much a data management project as it is a modelling assignment. In the exploratory analysis section, we first start by analysing the data to identify potential deterministic relationships. We also make sure to tidy up the data by transforming non-numerical variables and by tailoring certain variables to optimise a final model. This analysis is aided by some visualisations, which aid our interpretation of the data and variable selection. The model fitting consists of applying a forward stepwise regression i.e. we test if the addition of a variable improves our model's explanatory ability iteratively whilst controlling for/partialling out the effect of other variables. We apply a significance level of 5% throughout the analysis. We perform a log-linear regression which means that we transform the target variable (price for two people to stay in Vienna for four nights) to its natural log. This means we can interpret the coefficients on the explanatory variables as approximate expected percentage changes in price for four nights. This methodology has been well-researched, so our contribution is strictly empirical. 

## Results

Our optimal model reads as follows:
$Y = 3.64 - 0.44(Private\ Room) - 0.57(Shared\ Room) - 0.17(T5) - 0.13(T4) + 0.05(T2) + 0.38(Innere\ Stadt) + 0.29(Rating) + 0.09(Accommodates) + 0.14(Bathrooms) + 0.06(Superhost) + 0.01(Availability\_30) + \epsilon_i$
 - where Y = ln(price_4_nights).
 
This analysis is very approachable for the average person given the obvious logic behind each of the variables. We also note that we interpreted the log-linear coefficients as approximated percentage changes, though we understand that there will be slight deviations in reality from the approximated change and actual. We can interpret the following factors which influence the average expected price to stay in Vienna for four nights as follows:

 - Private Room: the negative coefficient shows that you should expect to pay less for a private room in a shared accommodation as opposed to a hotel room (which is the baseline). This signifies a negative premium (c. -44%) for having to share the other facilities in an accommodation. This is a dummy variable, i.e. is either one or zero if it is a private room or not.
 - Shared Room: this has a more negative coefficient than the Private Room which signifies that people on average should expect to pay less for a shared room than a private room, which makes sense (e.g. hostel vs hotel)
 - The variables T5, T4, T2, and Innere Stadt are location variables which represent pools of Vienna's 23 neighbourhoods grouped by mean rental price. Innere Stadt is the most expensive area by quite a lot, so is isolated. The higher the number in the other tiers, the less expensive AirBnB rental prices are on average. T3 is the assumed baseline category. It makes sense that neighbourhood should be an important explanatory factor of rental prices, as location is cornerstone of valuation drivers for real estate markets. The neighbourhoods were also better at explaining the variation in the price data than distance from the center of the city, which makes sense as changes in accommodation quality and demand isn't generally constant the further you go from the centre in unplanned cities
  - Rating: this variable denotes the AirBnB rating for the property out of five on the website. This is an obvious factor affecting price of accommodation as demand for higher rated properties should be higher on average than lower rated properties, as previous experiences there are expected to be better on average. This is especially important when travelers sort the accommodation options on the website in terms of rating which is often the case.
  - Accommodates: this coefficient reads that if an accommodation can fit one more person, on average, the rental price for four nights should increase by 9%. The direction of the variable makes sense; however, we would expect this relationship to be convex in general i.e. the difference between the price of an accommodation for two people versus three should be greater than for eleven versus ten, based on logic.
  - Bathrooms: an increase of one in the number of bathrooms should increase the average rental price for four nights by 14% on average, when controlling for the other variables. 
  - Superhost: a Boolean variable which is True if the host is a superhost. The positive relationship here makes sense as you would expect travelers to pay a premium on average to stay with a Super Host. This variable is unique to the AirBnB platform.
  - Availability_30: how many days the room is available in the next 30 days. This variable is an extremely useful gauge of current demand for the property. Logically, we would expect demand to be conditional on price as opposed to the structure above, but there appears to be some simultaneity between the two variables.


## Source the Data

### Load the Data

> We first load the data and parse the price variable into an integer.

```{r load_data, echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE}
# use cache=TRUE so you dont download the data everytime you knit
vienna_listings <- vroom("http://data.insideairbnb.com/austria/vienna/vienna/2021-09-12/data/listings.csv.gz") %>%
  clean_names()
```

```{r parse price}
# Price is a character, parse to integer
vienna_listings <- vienna_listings %>% 
  mutate(price = parse_number(price))

# In case of duplicate rows
unique_listings = unique(vienna_listings[,])
```

### Mapping 

> The following code plots on the map all AirBnBs where minimum_nights is less than equal to four. 

```{r, out.width = '80%'}

leaflet(data = filter(vienna_listings, minimum_nights <= 4)) %>% 
  addProviderTiles("OpenStreetMap.Mapnik") %>% 
  addCircleMarkers(lng = ~longitude, 
                   lat = ~latitude, 
                   radius = 1, 
                   fillColor = "blue", 
                   fillOpacity = 0.4, 
                   popup = ~listing_url,
                   label = ~property_type)
```

> Using this map, we can be redirected to any airbnb we want to check out along with an overview of their location. We know move on to exploratory analysis.


Data Exploration and Feature Selection (EDA)
===================================================================================================================================
    
We normally conduct a thorough EDA via three methods:

 - Looking at the raw values.
 - Computing summary statistics of the variables of interest, or finding NAs
 - Creating informative visualizations

However, we analyse the correlation matrices later in the process when we have narrowed down further our "meaningful" variables, i.e. those with a suspected possible significant explanatory ability of the variation of price.

## Variable Summary and Selection

### Glimpse Data

```{r, eval=FALSE}
# Use Glimpse from the Dplyr package to get an initial look at the raw data
kable(dplyr::glimpse(vienna_listings))
```

> Our data has 11583 rows/observations with 74 variables. 

### Skim Data

```{r, warning=FALSE,error=FALSE, eval=FALSE}
# Generate summary statistics with mosaic and skimr packages
kable(skimr::skim(vienna_listings))
```
> There are 37 independent numeric variables, 9 logical, 5 date and 22 character variables. 


## Data Wrangling

### Logical Selection of Variables

> Since there are 73 independent variables and many of them don't make any sense (eg. url) for price prediction, we select some variables for simplifying later exploration. According to our hypothesis, the following variables could potentially affect the price. 

```{r}
# Logical screening

vienna_listings_cleaned <- vienna_listings %>% 
  select(
    price,
    host_response_time,
    host_response_rate,
    host_acceptance_rate,
    host_is_superhost,
    host_total_listings_count,
    host_identity_verified,
    neighbourhood_cleansed, #23 areas
    latitude,
    longitude,
    property_type,
    room_type,
    bathrooms_text,
    bedrooms,
    accommodates,
    beds,
    minimum_nights,
    maximum_nights,
    availability_30,
    availability_365,
    instant_bookable,
    number_of_reviews_ltm,
    number_of_reviews,
    review_scores_rating,
    reviews_per_month
  )

```

```{r, eval=FALSE}
#glimpse and skim again
kable(glimpse(vienna_listings_cleaned))
skimr::skim(vienna_listings_cleaned) # kable is too slow with skim
```

### Analysis of Price

> Then, we plot the distribution of price. As the distribution is significantly right skewed, we use the log function to aptly transform the price data.

```{r}
# Visualise the Price
vienna_listings_cleaned %>% ggplot(aes(x=price)) + 
  geom_density()+labs(
    title = "Price PDF")+
  theme_economist()

```

```{r,out.width="100%"}
# Our data is very positively skewed which will make prediction more difficult, so we take the natural log of price to normalise the data
vienna_listings_cleaned <- vienna_listings_cleaned %>% 
  mutate(lnprice = log(price))

# Visualise the Data
vienna_listings_cleaned %>% ggplot(aes(x=lnprice)) + 
  geom_density()+labs(
    title = "Log Price PDF")+
  theme_economist()
```

```{r}
confint(vienna_listings_cleaned$price, mean, level = 0.9)
```

> As the log price is still a bit right skewed, we then plot the 5% and 95% percentile of the price. This gives us approximate price upper and lower bounds of €20 and €169 respectively.

```{r,out.width="100%"}
# Remove positive outliers and non-sensical/irrelevant values left of the mean to build a more meaningful analsysis
vienna_listings_cleaned <- vienna_listings_cleaned %>% 
  filter(between(lnprice, log(20), log(169)), na.RM=TRUE)

# Visualise the Data
vienna_listings_cleaned %>% ggplot(aes(x=lnprice)) + 
  geom_density()+labs(
    title = "Log Price PDF (Tails Removed)")+
  theme_economist()

```

```{r}
# save this dataset
vienna_listings_final <- vienna_listings_cleaned
```

> Now the natural log of price is approximatley normally distributed within these bounds; thus, we use lnprice as our dependent variable to construct our model.


## Transform to Numerical: Bathrooms, host_response_rate, host_acceptance_rate

> We then change the non-numerical variables such as bathrooms, host_response_rate, and host_acceptance_rate to numerical variables.

```{r,warning = FALSE,message = FALSE}
vienna_listings_final <- vienna_listings_final%>%
  mutate(bathrooms = parse_number(bathrooms_text),
         host_response_rate = parse_number(host_response_rate),
         host_acceptance_rate = parse_number(host_acceptance_rate))%>%
  select(-bathrooms_text)
```

```{r, eval=FALSE}
#glimpse and skim again
kable(glimpse(vienna_listings_final))
kable(skim(vienna_listings_final))
```  


### Characters to Factor Values  

```{r, }
# Create factor variables for host response time 
kable(unique(vienna_listings_final$host_response_time)) %>% 
  kable_styling()
invisible(vienna_listings_final %>% 
  mutate(
    # mark "N/A" as missing values
    host_response_time = ifelse(host_response_time =="N/A",NA,host_response_time),
    # relevel
    host_response_time = fct_relevel(host_response_time,
                                            "within an hour", 
                                            "within a few hours",
                                            "within a day",
                                            "a few days or more"
                                            )))
# Create factor variables for room types 
kable(unique(vienna_listings_final$room_type))%>% 
  kable_styling()
invisible(vienna_listings_final$room_type <- factor(vienna_listings_final$room_type, 
                                          labels = unique(vienna_listings_final$room_type)))

```
In the original raw data, there are 74 Variables with 11,583 Observations. After filtering the data based on log price (to remove NA price values and outliers), we have 10,602 observations. 

After variable selection based on logic, we have 26 variables.

 - The following variables are numbers: 
Numeric Variables * 19: host_total_listings_count, host_response_rate, host_acceptance_rate, accommodates, longitude, latitude, beds, bedrooms,bathrooms, variables for maximum and minimum nights, number of reviews, availabiity data, review scores data,  calculated host data etc. 

 - The following are categorical or factor variables: 
Logical/Boolean Variables * 3 : host_is_superhost, host_identity_verified, instant_bookable
Character variables * 4 : host_response_time, neighbourhood_cleansed, property_type, room_type


### Property Types

  > Next, we look at the variable property_type. We use the *count* function to determine how many categories there are their frequency. The top 4 most common property types are "Entire rental unit", "Private room in rental unit", " Entire condominium (condo)", and "Entire serviced apartment". The four types account for 91.19% proportion of the total listings.

```{r}
# Identify the amount of each property type
property_type_count <- vienna_listings_final %>%
    count(property_type) %>%
    mutate(percentage = n/sum(n)*100)%>%
    arrange(desc(n))

kable(property_type_count)

```




Top 4: 

 - Entire rental unit (65.78%)
 - Private room in rental unit (18.06%)
 - Entire condominium (condo) (4.57%)
 - Entire serviced apartment (2.76)

Since the vast majority of the observations in the data are one of the top two property types (84%), we would like to combine Entire condominium (condo), Entire serviced apartment and Entire loft as a type called Entire apartment. This method was better suited to the Vienna dataset, we require sufficient datapoints for each category in order to reduce our model's standard errors.

> We then create a simplified version of property_type variable that has 4 categories: the top two categories, Entire apartment and Other.

```{r}
# First we need to summarize the other values in the Category "Others"
vienna_listings_final <- vienna_listings_final %>%
  mutate(prop_type_simplified = case_when(
    property_type %in% c("Entire rental unit","Private room in rental unit") ~ property_type, 
    property_type %in% c( "Entire condominium (condo)", "Entire serviced apartment","Entire loft") ~ "Entire apartment",
    TRUE ~ "Other"
  ))
```


```{r}
# Check if done correctly
kable(vienna_listings_final %>%
  count(property_type, prop_type_simplified) %>%
  arrange(desc(n)))

```  


> Next we can make a factor out of prop_type_simplified.   

```{r}
vienna_listings_final <- vienna_listings_final %>% 
  mutate(
     prop_type_simplified = fct_relevel(prop_type_simplified,
                                 "Entire rental unit",
                                 "Private room in rental unit",
                                 "Entire apartment",
                                 "Other")) %>%
  select(-property_type)
```


### Analysis of Minimum Nights

> We only want to include listings in our regression analysis that are intended for travel purposes, as this is the target user for our model.

```{r}
# right skewed
vienna_listings_final %>% 
  ggplot(aes(x=minimum_nights))+
  geom_histogram() +
  theme_economist() +
  labs(title="Minimum Nights Distribution")

vienna_listings_final %>% 
  filter(minimum_nights<=10) %>%
  mutate(minimum_nights=as.integer(minimum_nights)) %>%
  ggplot(aes(x=minimum_nights))+
  geom_bar()+
  scale_x_continuous(breaks = seq(0,10,1)) +
  theme_economist() +
  labs(title="Minimum Nights Distribution (Outliers Removed)")
```

- The most common values for the variable minimum_nights is 1,2,3.
- 1 minimum night stands out among the common values.
- The seemingly unusual large values for minimum_nights probably represent listings for long-term tenants, not for travelers.

> Filter the airbnb data so that it only includes observations with minimum_nights <= 4

```{r}
vienna_listings_final <- vienna_listings_final %>% 
  filter(minimum_nights<=4) %>% 
  filter(maximum_nights>=4) # Since we will be modelling the price to stay for four nights, maximum stay must be greater than or equal to 4
invisible(vienna_listings_final)
```

> We now have only 8,844 listings.

### Analysis of Location: Neighbourhood, Longitude and Latitude

> We want to leverage the longitude and latitude variables by calculating the distance between every listing and center of Vienna. We take postcode 1010 as the center of Vienna, which we confirmed with a local citizen.

```{r}
# Calculate the distance, meters as unit
vienna_listings_final <- vienna_listings_final %>% 
  rowwise() %>% 
  mutate(
    dist_from_cent = distm(c(latitude, longitude), c(48.208604427334215, 16.37353507157435), 
                      fun = distVincentyEllipsoid)[1,1]
  ) 
```

> We then compare median distance of each neighbourhood. Use median to avoid outliers.

```{r}
dist_neighbourhood <- vienna_listings_final %>% 
  group_by(neighbourhood_cleansed) %>% 
  summarise(
    med_dist = median(dist_from_cent),
    med_price = median(price),
    count_listings = n()
  ) %>% 
  arrange(med_dist)

kable(dist_neighbourhood) %>% 
  kable_styling()
```

> We now plot Neighbourhood median price against median distance from city center

```{r, fig.width=12, fig.height=8}
dist_neighbourhood %>% 
  ggplot(aes(x=med_dist,y=med_price,size=count_listings))+
  geom_point(color="#1A5276",alpha=0.4)+
  geom_text_repel(aes(label=neighbourhood_cleansed),size=4)+
  scale_size_continuous(range=c(4,20))+
  theme_bw()+
  theme(
        plot.title =element_text(size=16, face='bold',hjust = 0,margin = margin(10,0,10,0)),
        plot.subtitle =element_text(size=16, hjust = 0), 
        axis.text.x = element_text(size=10),
        axis.text.y = element_text(size=12),
        axis.ticks.x = element_line(),
        axis.ticks.y=element_line(),
        axis.title.x = element_text(size=12,face='bold'),
        axis.title.y = element_text(size=12,face='bold'),
        ) +
  labs(title = "Neighbourhood median price against median distance from city center", 
       x="Median distance from city center",y="Median Price")+
  theme_economist_white()
```

Innere Stadt is the neighbourhood located right at city center (587.45m in average); thus, it exhibits a high average price.

Neighbourhoods located less than 6 km to city center offer listings for price in range 55 to 65, with price decreasing when distance increases on average.

Interestingly, listings located more than 9km away have higher price for 60-70. There appears to be no certain pattern/relationship between distance from the center and price, but we will investigate further later.

```{r, warning=FALSE, message=FALSE,error=FALSE}
# Grouping by neighbourhood_cleansed
neighbourhood_sorted = vienna_listings_final %>% 
 group_by(neighbourhood_cleansed) %>% 
  summarise(mean_price = mean(price)) %>% 
  arrange(-mean_price)

kable(neighbourhood_sorted) %>% 
  kable_styling()
```

We would like to divide our neighbourhoods into five different areas, as for the 23 we currently have, estimation error embedded in our final model would be too high out of sample, potentially giving our out-of-sample predictions massive standard errors. We decide to group the areas by similar mean prices. Since Innere Stadt is such an outlier in terms of price, we divide the rest of the areas into four groups (5,6,6,5). 

Note that however, we get unicode characters in three of the neighbourhood_cleansed variables. However, we can avoid any hassle and simply call the variable from the table of neighbourhood_cleansed mean prices. We will later assess our groupings to see if they are statistically significant predictors of price.

```{r,error=FALSE,warning=FALSE}
# First we need to summarize the other values in the Category "Others"
vienna_listings_final <- vienna_listings_final %>%
  mutate(neighbourhood_simplified = case_when(
    neighbourhood_cleansed %in% c("Innere Stadt") ~ neighbourhood_cleansed, 
    neighbourhood_cleansed %in% c("Wieden", "Landstra§e", "Mariahilf", "Neubau", "Liesing") ~ "T2",
    neighbourhood_cleansed %in% c("Leopoldstadt", "Alsergrund", "Hietzing", "Donaustadt", "Josefstadt") ~ "T3",
    neighbourhood_cleansed %in% c(neighbourhood_sorted[12,1], "Brigittenau", "Favoriten", neighbourhood_sorted[15,1], "Margareten", "Floridsdorf") ~ "T4",
    neighbourhood_cleansed %in% c(neighbourhood_sorted[18,1], "Meidling", "Simmering", "Penzing", "Hernals", "Ottakring") ~ "T5"
  ))

# Now we need to factorise the new variable
vienna_listings_final <- vienna_listings_final %>% 
  mutate(
     neighbourhood_simplified = fct_relevel(neighbourhood_simplified,
                                 "Innere Stadt",
                                 "T2",
                                 "T3",
                                 "T4",
                                 "T5")) %>%
  select(-neighbourhood_cleansed)

```


```{r}
# Do boxplot for each neighbourhood simplified
vienna_listings_final %>% 
  filter(!is.na(neighbourhood_simplified)) %>% 
  ggplot(aes(y=price)) + 
  geom_boxplot(aes(x=fct_reorder(neighbourhood_simplified,price,median,.desc=TRUE)))+
  labs(title = "Median Price by Neighbourhood")+
  theme_economist()+
  theme(
    axis.title.x= element_blank()) # have to put this after we use economist theme or it puts the x axis title back
  
```

We can see see the gradual decline in median price from T2 to T5, but, again, Innere Stadt really stands out as the most in demand location for travelers visiting Vienna. As we saw earlier however, the median price for the locations doesn't appear to be dependent on location in general. We cross-checked the logic of this with our resident Vienna group member, who confirmed that some of the nicest areas which are generlly in quite high demand from tourists are outside the city centre. Since Vienna's attractions lie in its architecture and general beauty, we can assume that it is probably a more mature tourist audience, who may want to avoid the hustle and bustle of the city, seeing as nightlife isn't the primary attraction of Vienna. 

> We can reasonably delete longitude and latitude from dataset now.

```{r}
vienna_listings_final <- vienna_listings_final %>% 
  select(-c(longitude,latitude))
```


### Target Variable

> For the target variable $Y$, we will use the cost for two people to stay at an Airbnb location for four nights. We will create a new variable called `price_4_nights` that uses `price`, and `accomodates` to calculate the total cost for two people to stay at the Airbnb property for 4 nights. This is the variable $Y$ we want to explain.

```{r}
vienna_listings_reg <- vienna_listings_final %>% 
  filter(accommodates>=2) %>%
  mutate(price_4_nights = price * 4,
         lnprice_4_nights = log(price_4_nights))
```

```{r}
# Visualise the Price
vienna_listings_reg %>% ggplot(aes(price_4_nights)) + 
  geom_density()+
  labs(title = "Price for Four Nights")+
  theme_economist()
```

```{r}
# Visualise the Price
vienna_listings_reg %>% ggplot(aes(x=lnprice_4_nights)) + 
  geom_density()+
  labs(title = "Log Price for Four Nights")+
  theme_economist()
```

> We use log(price_4_nights) for the regression model for the reasons mentioned earlier. The positive skew in the price_4_nights variable makes it more difficult to predict extreme values of price, relative to the normalised natural logarithm.


### Deal with Missing/Unreliable Values

> There some missing values in review_scores_rating, which we are quite confident will be a significant predictor. We decide to remove these rows (the lm regression in R would remove these automatically but we prefer to do it now to have the data cleaner). We also decide that it makes sense to remove very new or "unrated" properties as data on reviewed properties will on average be much more reliable, and we still have a sufficient number of datapoints to formulate a model.


```{r}
vienna_listings_reg <- vienna_listings_reg %>%
  filter(!is.na(review_scores_rating)) %>% 
  filter(number_of_reviews>=10)
```


## Split the Dataset into Training and Testing

```{r Sampling}
# Set seed so we will get the same results
set.seed(202110)

# Split our data 75% as train and 25% as test
train_test_split <- initial_split(vienna_listings_reg, prop=0.75)
vienna_train <- training(train_test_split)
vienna_test <- testing(train_test_split)
```


Model Selection and Validation
===================================================================================================================================

## Regression Analysis

Model selection is a structured process. We employ a forward regression process, as we test the significance of variables whilst controlling for others in order to avoid including variables with high collinearity. We first assess the correlation matrix of our numerical explanatory variables in order to determine which variables have the strongest relationships with the target variable, and to highlight variables with high collinearity so as to avoid multicollinearity issues amongst the regressors. We then iterate through a number of models in order to determine if the increase in adjusted r squared is worth the increase in exposure to estimation error in a stepwise forward regression processz. When building regression models with a large number of potential independent variables, we must be cautious of the tradeoff between in-sample bias and out of sample variance of predictions. To optimise our model based on this tradeoff, we can compare the adjusted R squared against the out of sample RMSE (from the testing data). We also perform other model diagnostics such as analysis of the residuals, in order to ensure that they are approximately normal and homoskedastic with a mean of zero.

### Correlation of Numerical Variables

```{r, correlation, fig.width=15, fig.height=12}
# Scatterplot/Correlation matrix for numerical variables
vienna_train %>% 
  select(where(is.numeric)) %>% 
  select(-price )%>%
  ggpairs()+
  theme_economist()
```


This correlogram gives us a better indication of which variables may explain more of the target variable, which we will use to gear our forward regression in the right direction.

We can also see there are many variables which have a strong correlation with each other, eg. beds and accommodates. This means we need to consider multicollinearity in the regression amongst the predictors. 

However, some scatterplots cannot support a linear relationship between variables, eg. price and total listings of the host. There could be some correlations  conditional on the value of a categorical variable also which we need to be careful of, eg. the correlation between price and bedrooms could be conditional on property type or room type, since whether the listing is entire or a private room for rental might influence the number of bedrooms. For this reason we simply use the correlogram as a limited source of guidance to help us start off in the right direction


### Model 1

> We fit regression model called model1 with the following explanatory variables: prop_type_simplified, number_of_reviews, and review_scores_rating. 

```{r}
model1 <- lm(lnprice_4_nights ~ prop_type_simplified+number_of_reviews+review_scores_rating, data = vienna_train)
par(mfrow=c(2,2)) 
msummary(model1)
kable(car::vif(model1)) %>% 
  kable_styling()
autoplot(model1) +
  theme_minimal() + 
  labs (title = "Model 1 Diagnostic Plots")
```

Metrics:

 - The p-value of model1 is smaller than 0.05, which means it is a significant regression model that produces a significant relationship to predict the log of the cost for 2 people and 4 nights at Vienna. The adjusted R-squared is 0.2368 which indicates that this model has limited explanatory power.
 - The residual standard error is quite high also at c. 0.4
 - We later look at the AIC and out-of-sample RMSE of the models, seeing as these are generally used for comparison
 - We use the vif() function to check for multicollinearity between the regressors. We get low VIF values here so it shouldn't be an issue amongst these variables going forward

The Coefficients:

 - Our intercept here is 4.377. When we convert this to price it gives us a value of almost 80, which also shows that we must be omitting a large number of determinants of price
 - The review_scores_rating coefficient is significant, indicating that the rating scores of reviews on a listing have a significant impact on costs. To be specific, with other variables same, if the review score increase by 1, we expect the cost for 2 people and 4 nights at Vienna increases by approximately 2.56% (property of log-linear regression)
 - The coefficient of number_of_reviews is significant at the 5% level of significance, but is very tiny
 - The coefficient of prop_type_simplified can be interpreted as dummy variables. The baseline category is Entire rental unit. Thus, the other three dummy variables' represent the deviation in mean log of price of four night stay of the three other property types
- The coefficient for Private room in rental unit is significant, indicating that the cost for a Private room in rental unit is significantly lower than cost for an Entire rental unit. This makes sense that the coefficient is negative because private rentals are meant to share kitchens and bathrooms with other people, thus leading to lower costs. To be specific, with other variables same, a Private room in rental unit costs approx. 62.4% less than an Entire rental unit (properties of log-linear regression)
- Coefficient for Entire apartment is significant, indicating that the cost for a Entire apartment is significantly higher than cost for an Entire rental unit. This makes sense that the coefficient is positive because Entire rental unit in this dataset is not specified and maybe worse than a Entire apartment. To be specific, with other variables same, an Entire apartment costs 6.02% more than an Entire rental unit.
- Coefficient for Other properties is significant, indicating that cost for other properties is significantly lower than an Entire rental unit. This is probably because this category includes a mix of properties like share rooms. To be specific, with other variables same, Other properties costs 17.91% less than an Entire rental unit


### Model 2

> We now want to determine if room_type is a significant predictor of the cost for 4 nights, given everything else in the model. 

```{r}
model2 <- lm(lnprice_4_nights ~ prop_type_simplified+room_type+number_of_reviews+review_scores_rating, data = vienna_train)
msummary(model2)
kable(car::vif(model2)) %>% 
  kable_styling()
autoplot(model2) +
  theme_minimal() + 
  labs (title = "Model 2 Diagnostic Plots")
```

 - The adjusted R squared is 0.2542, which is a marginal improvement on the previous model at the cost of three extra sources of estimation error, which will increase the variance of our predicted means

 - We also get very high variance inflation factors for property type and room type, understandably. This is also indicated by the deviation of the previously significant property coefficient for private room in rental unit to being insignificant. Given that the room type variable only very marginally improved the model's explanatory value, and introduces a lot of multicollinearity amongst the regressors, we decide to drop either the room_type or property_type variable. When we tested the above model with either room_type or prop_type_simplified, the room_type regression produced a higher standard error. We also prefer the room_type categories as they are more applicable to how a user would appraise a property
 

### Model 3

> We want to test if the number of bathrooms, bedrooms, beds, or size of the house (accomodates) are significant predictors of price_4_nights. However, we suspect a high degree of multicollinearity betwen these variables, so we make a correlation matrix between these variables to determine if there is a high corelation between the predictors, and which one demonstrates the strongest relationship with the target variable.

```{r}
vienna_train %>% 
  select(lnprice_4_nights, bathrooms, bedrooms, accommodates, beds) %>% 
  ggpairs()+
  labs(title = "Correlation Matrix") 
```

 - Excluding bathrooms, there is a high degree of correlation between the predictors, and accommodates demonstrates the strongest relationship with the target variable. This makes sense, seeing as accommodates is determined by the number of beds and bedrooms in a property. Thus, we decide to only add in the Accommodates variable into our model. 

 - Bathrooms demonstrates a relatively weak correlation coefficient with accommodates, but logically you would expect an accommodation with more bathrooms to cost more per person, when we control for the accommodates factor. Price is probably more sensitive to this variable  when the number for accommodates is very high however.

```{r}
model3 <- lm(lnprice_4_nights ~ room_type+number_of_reviews+review_scores_rating
             +accommodates+bathrooms, data = vienna_train)
msummary(model3)
kable(car::vif(model3)) %>% 
  kable_styling()
autoplot(model3) +
  theme_minimal() + 
  labs (title = "Model 3 Diagnostic Plots")
```

 - Adjusted R-squared is 0.3743, greatly improved by factoring in accommodates and bathrooms, both of which are statistically significant ($\alpha$ =5%).

 - Note the low multicollinearity amongst the regressors after the removal of the property type variable.


### Model 4

> We now want to test if superhosts command a pricing premium, after controlling for other variables.

```{r}
model4 <- lm(lnprice_4_nights ~ room_type+number_of_reviews+review_scores_rating
             +accommodates+bathrooms+host_is_superhost, data = vienna_train)
msummary(model4)
kable(car::vif(model4)) %>% 
  kable_styling()
autoplot(model4) +
  theme_minimal() + 
  labs (title = "Model 4 Diagnostic Plots")
```

 - Adjusted R-squared is 0.38, a barely improved by including the Boolean host_is_superhost variable. However, this regressor is statistically significant, and positive as expected. We decide to continue with the variable as it is extremely significant, and wait until we control for some additional regressors.

 - Note that the lm() function automatically discards rows with missing values for the explanatory variables. We were reluctant to remove some of these rows earlier, as we did not know if some of these variables would be statistically significant predictors of the target variable,; and thus, we may have discarded valuable data unnecessarily if they were not useful.


### Model 5

> We now want to test if the offering by some hosts allowing you to immediately book their listing is a significant predictor of the taget variable after controlling for other variables.

```{r}
model5 <- lm(lnprice_4_nights ~ room_type+number_of_reviews+review_scores_rating+accommodates+bathrooms+host_is_superhost+instant_bookable, data = vienna_train)
msummary(model5)
kable(car::vif(model5)) %>% 
  kable_styling()
autoplot(model5) +
  theme_minimal() + 
  labs (title = "Model 5 Diagnostic Plots")
```

 - The adjusted R squared is unchanged from Model 4, and we have an insignificant predictor in instant_bookable, so we discard this variable going forward.
 

### Model 6

> We now test the predictive ability of the neighbourhood_simplified variable which we created earlier. 

```{r}
model6 <- lm(lnprice_4_nights ~ room_type+neighbourhood_simplified+ number_of_reviews+review_scores_rating+accommodates+bathrooms+host_is_superhost, data = vienna_train)
msummary(model6)
kable(car::vif(model6)) %>% 
  kable_styling()
autoplot(model6) +
  theme_minimal() + 
  labs (title = "Model 6 Diagnostic Plots")+
  theme(text = element_text(size=6))
```

 - The introduction of the neighbourhood_simplified variable improves the adjusted R squared greatly to 0.48. Each bucket we computed is a statistically significant dummy variable at the 5% level of significance. However, we lose 362 additional observations. We decide that the increase in explanatory power is worth the loss in data as we still have a sufficient degrees of freedom to avoid over-fitting.

 - The inclusion of the neighbourhood_simplified variable however results in the "Entire home/apt" category of the room_type variable to become statistically insignificant i.e. the mean of the target variable is not statistically significantly different for an entire home/apartment than the baseline room_type in the model (Hotel room) when controlling for other variables. So we should group this category into the baseline category based on this. There must exist a relationship between the neighbourhoods and the room_type variable, which is understandable as more central areas will have more hotels and apartments on average when compared with areas outside/on the outskirts of the city.


```{r, warning=FALSE,error=FALSE,message=FALSE}
# First we need to summarize the specified room_type category into the baseline for training and testing dataset, then we re-factorise them

vienna_train <- vienna_train %>%
  mutate(room_types = case_when(
    room_type %in% c("Hotel room", "Entire home/apt") ~ "Entire property", 
    room_type %in% c("Private room") ~ "Private room",
    room_type %in% c("Shared room") ~ "Shared room"
  ))

# Now we need to factorise the new variable
vienna_train <- vienna_train %>% 
  mutate(
     room_types = fct_relevel(room_types,
                                 "Entire property",
                                 "Private room",
                                 "Shared room"))

vienna_test <- vienna_test %>%
  mutate(room_types = case_when(
    room_type %in% c("Hotel room", "Entire home/apt") ~ "Entire property", 
    room_type %in% c("Private room") ~ "Private room",
    room_type %in% c("Shared room") ~ "Shared room"
  ))

# Now we need to factorise the new variable
vienna_test <- vienna_test %>% 
  mutate(
     room_types = fct_relevel(room_types,
                                 "Entire property",
                                 "Private room",
                                 "Shared room"))
```

> Now we fit model 6 again with the new room_type grouping.

```{r}
model6 <- lm(lnprice_4_nights ~ room_types+neighbourhood_simplified+ number_of_reviews+review_scores_rating+accommodates+bathrooms+host_is_superhost, data = vienna_train)
msummary(model6)
kable(car::vif(model6)) %>% 
  kable_styling()
autoplot(model6) +
  theme_minimal() + 
  labs (title = "Model 6 Diagnostic Plots")+
  theme(text = element_text(size=6))
```

 - All of our coefficients on the regressors are now statistically significant. The standard error on the Entire home/apt category coefficient was very large, due to a lack of data.


### Model 7

> We have been using the number of reviews variable, but we are cautious that it may not be the most current reviews variable in the dataset, especially considering its miniscule coefficient. We now want to select which of the review number variables we should use. We look at the correlation matrix between number_of_reviews, reviews_per_month, number_of_reviews_ltm. We also want to test the effect of including the avalability_30 variable on the model's explanatory power, as it could be a proxy for general demand.

```{r}
vienna_train %>% 
  select(lnprice_4_nights, number_of_reviews, reviews_per_month, number_of_reviews_ltm) %>% 
  ggpairs()+
  labs(title = "Correlation Matrix")
```

 - The number of reviews over the last twelve months is a more current variable than the number of reviews, and demonstrates a stronger correlation with the target variable. We would only like to use one of these variables at most due to the high collinearity between them, and because we want to minimise out-of-sample variance of our mean forecasts. However, the explanatory values of these variables are pretty poor. We tested Model 6 with the number_of_reviews_ltm variable, but it only increased the adjusted R squared by less than 0.01. When we removed the number_of_reviews variable also, the adjusted coefficient of determination barely changed, so we discard this variable on the basis that the additional explanatory value is not worth the increase in model variance and estimation error. It is surprising that the number of reviews has approximately no relationship with the target variable. 

> We now fit the model with availability_30 included and number of reviews removed.

```{r}
model7 <- lm(lnprice_4_nights ~ room_types+neighbourhood_simplified+review_scores_rating+accommodates+bathrooms+host_is_superhost+availability_30, data = vienna_train)
msummary(model7)
kable(car::vif(model7)) %>% 
  kable_styling()
autoplot(model7) +
  theme_minimal() + 
  labs(title = "Model 7 Diagnostic Plots")+
  theme(text = element_text(size=6))
```

 - Availability_30 is statistically significant at the 5% level of significance. It also increases our model's adjusted R squared by over 0.04 to 0.52, which is a welcome boost. Multicollinearity is relatively low in this model as well which gives us confidence in the approximate independence of our explanatory variables. If availability_30 increases by 1, we would expect, based on the model, that price for two people for four nights would increase by c. 1% on average. This variable is statistically significant at the 5% level of significance.

 - After running the specified regressions, we must look at the remaining variables we have not used so far to potentially optimise our models explanatory value further.

```{r}
colnames(vienna_train %>% 
  select(-c(lnprice_4_nights,prop_type_simplified,neighbourhood_simplified,review_scores_rating,accommodates,host_is_superhost,availability_30,number_of_reviews, reviews_per_month, number_of_reviews_ltm,instant_bookable,bathrooms, bedrooms, beds,room_type, minimum_nights, maximum_nights, lnprice, dist_from_cent, price_4_nights, availability_365,price,room_types)))

```
 

Relevant variables not tested yet:

 - host_identity_verified
 - host_total_listings_count (not logically deterministic of price)
 - host_acceptance_rate (too many missing entries)
 - host_response_rate (too many missing entries)
 - host_response_time (too many missing entries)


### Model 8

We decide to test the host_identity_verified Boolean variable, as we can logically hypothesise that people could potentially pay a premium for the security of the booking. However, the host may not demand a premium for being verified, since it is strictly the seller which determines market price here.

```{r}
model8 <- lm(lnprice_4_nights ~ room_types+neighbourhood_simplified+review_scores_rating+accommodates+bathrooms+host_is_superhost+availability_30+host_identity_verified, data = vienna_train)
msummary(model8)
kable(car::vif(model8)) %>% 
  kable_styling()
autoplot(model8) +
  theme_minimal() + 
  labs (title = "Model 8 Diagnostic Plots")+
  theme(text = element_text(size=6))
```

 - The host identity verification variable is not statistically significant at the 5% level of significance, so we discard it. 
 
 - We have now exhausted our supply of explanatory variables. We can say so far that Model 7 is the best in terms of adjusted R squared so far, but we will compare the models further below using AIC, residual standard error, and out-of-sample RMSE after. We will then perform diagnostics tests on the residuals, as if the distribution of the residuals violates our OLS assumptions, we must re-select the models, or perform heteroskedasticity and autocorrelation robust regressions.

```{r}
models_compare <- huxreg(model1, model2, model3, model4, model5, model6, model7,model8,
                 statistics = c('#observations' = 'nobs', 
                                'R squared' = 'r.squared', 
                                'Adj. R Squared' = 'adj.r.squared', 
                                'Residual SE' = 'sigma',
                                'AIC' = 'AIC'), 
                 bold_signif = 0.05, 
                 stars = NULL
) %>% 
  set_caption('Comparison of models')
models_compare

# Get list of adjusted R squared values
ar2_comparison = data.frame(summary(model1)$adj.r.squared,summary(model2)$adj.r.squared,summary(model3)$adj.r.squared,summary(model4)$adj.r.squared,summary(model5)$adj.r.squared,summary(model6)$adj.r.squared,summary(model7)$adj.r.squared,summary(model8)$adj.r.squared)
```


```{r}
# Get out of sample RMSE
RMSE1 = 100*sd((predict(model1, vienna_test)-vienna_test$lnprice_4_nights),na.rm=TRUE)
RMSE2 = 100*sd((predict(model2, vienna_test)-vienna_test$lnprice_4_nights),na.rm=TRUE)
RMSE3 = 100*sd((predict(model3, vienna_test)-vienna_test$lnprice_4_nights),na.rm=TRUE)
RMSE4 = 100*sd((predict(model4, vienna_test)-vienna_test$lnprice_4_nights),na.rm=TRUE)
RMSE5 = 100*sd((predict(model5, vienna_test)-vienna_test$lnprice_4_nights),na.rm=TRUE)
RMSE6 = 100*sd((predict(model6, vienna_test)-vienna_test$lnprice_4_nights),na.rm=TRUE)
RMSE7 = 100*sd((predict(model7, vienna_test)-vienna_test$lnprice_4_nights),na.rm=TRUE)
RMSE8 = 100*sd((predict(model8, vienna_test)-vienna_test$lnprice_4_nights),na.rm=TRUE)

RMSE_compare = data.frame(RMSE1, RMSE2, RMSE3, RMSE4, RMSE5, RMSE6, RMSE7, RMSE8)

kable(RMSE_compare)%>% 
  kable_styling()

```



**Adjusted R-Squared**

 - Model 7 demonstrates the highest coefficient of determination

**Residual Standard Error**

 - Model 7 demonstrates the lowest residual standard error amongst the models, which should give us narrower prediction intervals as desired

**AIC**

 - Model 7 demonstrates the lowest AIC amongst the models. The akaike information criterion (AIC) is a very useful metric when determining if the addition of additional explanatory variables in a multivariate model justifies the addition of another estimated parameter. It includes a loss function which penalises the model based on the number of coefficients/parameters estimated

**Out-of-sample RMSE**

 - Model 7 demonstrates the lowest out-of-sample RMSE
 
**Residuals**

 - Model 7's residuals appear to be randomly scattered around its mean of 0, and look approximately normal as they appear to simulate a white noise distribution with no clear pattern as the target variable varies. The QQ plot is approximately along the 45 degree line also, with most residual quantiles well-fitted (note some deviation for extreme values however). It does not appear that we require heteroskedasticity robust standard errors.

 - Therefore, we decide to select model 7 as our optimal model for the price for two people to stay for four nights in Vienna. On average, this model should give us the most accurate predictions for average property rental price for the given input variables with the lowest standard errors amongst the fitted models.


Findings and Recommendations
===================================================================================================================================

## Model Interpretation

Finally, we use our optimised model for predictions. The model reads:

$Y = 3.64 - 0.44(Private\ Room) - 0.57(Shared\ Room) - 0.17(T5) - 0.13(T4) + 0.05(T2) + 0.38(Innere\ Stadt) + 0.29(Rating) + 0.09(Accommodates) + 0.14(Bathrooms) + 0.06(Superhost) + 0.01(Availability\_30) + \epsilon_i$

 - where Y = ln(price_4_nights)
 
Interpretations of Coefficients (when controling for other variables)

 - Private Room: the negative coefficient shows that you should expect to pay c. 44% less for a private room in a shared accommodation as opposed to a hotel room for four nights (which is the baseline)
 
 - Shared Room: you should expect to pay c. 57% less for a private room in a shared room as opposed to a hotel room (which is the baseline) for four nights

Logic: travelers pay a premium for privacy

 - T5: you should expect to pay c. 17% less to stay in a T5 neighbourhood (bottom 5 nighbourhoods by mean price) for four nights than a T3 neighbourhood (baseline)
 
 - T4: you should expect to pay c. 13% less to stay in a T4 neighbourhood for four nights than a T3 neighbourhood (baseline)
 
 - T2: you should expect to pay c. 5% more to stay in a T2 neighbourhood for four nights than a T3 neighbourhood (baseline)
 
 - Innere Stadt: you should expect to pay c. 38% more to stay in the Innere Stadt neighbourhood for four nights than a T3 neighbourhood (baseline)
 
Logic: location is one of the primary considerations for travelers staying in Vienna

  - Rating: you should expect to pay c. 29% more on average to stay in a property with a rating of one point higher than another for four nights, all else equal 

  - Accommodates: you should expect to pay c. 9% more on average to stay in a property which can accommodate one more person than another for four nights, all else equal
  
  - Bathrooms: you should expect to pay c. 14% more on average to stay in a property with one more bathroom than another for four nights, all else equal
  
  - Superhost: you should expect to pay c. 6% more to stay in a property listed by a superhost on average, all else equal
  
  - Availability_30: you should expect to pay c. 1% more on average to stay in a property which is available for one more day than another property in the next thirty days, all else equal

Logic: overpriced accommodation is likely to be a deterrent of demand
  

## Prediction

> We first predict the price for a private room, with at least 10 reviews, and an average rating of at least 90 (4.5/5.0). We also assume that the property accommodates 2 people, has one bathroom, and has the average availability_30 in the entire cleaned dataset. We predict for each of the five neighbourhood buckets from most to least expensive We use the model to predict the total cost to stay at this Airbnb for 4 nights.  

```{r,warning=FALSE}
imaginary_stay1 <- data_frame(room_types="Private room",
                              neighbourhood_simplified = c("Innere Stadt","T2", "T3", "T4", "T5"), # vary at will
                              review_scores_rating = 4.5:5,
                              accommodates=2,
                              bathrooms=1,
                              host_is_superhost=TRUE,
                              availability_30=mean(vienna_listings_reg$availability_30))

pred1=data.frame(exp(predict.lm(model7, imaginary_stay1,  interval = "confidence")), row.names = c("Innere Stadt","T2", "T3", "T4", "T5"))
kable(pred1)%>% 
  kable_styling()
```

> The table above gives the 95% confidence interval and expectation of the price for two people to stay for four nights with the other criteria mentioned for each of the simplified neighbourhood buckets. This demonstrates the significant difference between the neighbourhoods, with Innere Stadt the obvious positive outlier in terms of price. This table gives the user the location/price trade-off of staying in a nicer or potentially better located area, with the given assumptions above.

```{r, fig.width=10, fig.height=4}
#plotting graph using geom_errorbar
difference_pred1<- pred1 %>% 
  ggplot(aes(color=row.names(pred1))) +
  geom_errorbar(aes(y=row.names(pred1),xmin=lwr,xmax= upr),width=0.1,size=1.5,show.legend = FALSE) +
  geom_point(aes(x=fit,y=row.names(pred1)),size=5,show.legend = FALSE)+
  geom_text(aes(label=round(lwr,digits=2),x=round(lwr,digits=2),y=row.names(pred1)),size=3,color="black",hjust=0.5,vjust=-0.75,nudge_x=0.01,nudge_y=0.08,fontface = "bold")+
  geom_text(aes(label=round(upr,digits=2),x=round(upr,digits=2),y=row.names(pred1)),size=3,color="black",hjust=0.5,vjust=-0.75,nudge_x=0.01,nudge_y=0.08,fontface = "bold")+
  geom_text(aes(label=round(fit,digits=2),x=round(fit,digits=2),y=row.names(pred1)),size=3,color="black",hjust=0.4,vjust=-0.75,nudge_x=0.01,nudge_y=0.08,fontface = "bold")+
  theme(legend.position="none",legend.title = element_blank())+
  labs(
    title = "Prediction by Grouped Neighbourhood - Vienna",
    subtitle = "95% Confidence Intervals",
    x = "Prediction Price for Two People for Four Nights",
    y = "Neighbourhoods"
    )+
  theme_economist()+
  NULL

difference_pred1
```

> Note that the 95% confidence intervals for the predictions for T5 and T4 overlap, as do T3 and T2. However, they are statistically significant predictors so we are not alarmed. 


## Potential Improvements

Though we are quite happy with our results and the explanatory power of the final model, we must note potential improvements to the process/model:

### Obtain cleaner data
The data obtained was flawed (though not fatally flawed!) in numerous ways. The biggest issue with the data was the volume of missing values for potential explanatory variables, which can increase the standard errors of the parameter estimates. Ideally, we would like to maximise the amount of datapoints when fitting our model, but unfortunately this was negated by the inclusion of certain independent variables. The amount of transformations required and outliers in the data also can increase our margin of error in fitting and interpreting the data.

### Subjectivity
The subjectivity involved in truncating the tails of the data can take away from the scientific method employed. Other sources of subjectivity/bias include the grouping of categories when simplifying variables and omitting properties with fewer than ten reviews. This non-exact, though logic-based, decision-making can lead to bias in the model.

### Overfitting and Estimation Error
The inclusion of twelve coefficient estimates (including the intercept) leaves us prone to over-fitting errors which can increase out-of-sample variance in the residuals of the model. It also leaves us prone to estimation error which can void a model's usefulness in real life. However, we exercised caution consistently throughout the model selection process when analysing the tradeoff between bias and variance when including additional regressors.



# Acknowledgements

- The data for this project is from [insideairbnb.com](insideairbnb.com)
